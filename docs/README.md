# TranArc v0.1 - WIP Transformer Model
TranArc (Transformer-Architecture) is a simple work in progress neural network, making usage of self-attention mechanisms to process sequential data. Currently experimenting with ways to levarage these mechanisms to understand patterns and dependencies in data, whilst currently in development it acts a test of my understanding around the topic of machine learning.

## Dependencies
**Eigen**: C++ template library for linear algebra.
**Eigen Unsupported**: C++ module contributions from various users, using CXX11/Tensor.

## Referencing
This project could not be tackled alone, this would not be possible without:
[Attention Is All You Need](https://arxiv.org/pdf/1706.03762)
[An Introduction to Transformers](https://arxiv.org/pdf/2304.10557)

## Note
This is a work in progress and as it stands a mere experimentation with making transformers and GPT adjacent neural network models. Currently not intended for any specific usage, but could be applied to tasks such as; NLP, NER, Image Processing, etc... This project may or may not be completed eventually.